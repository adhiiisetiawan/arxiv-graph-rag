Bridging Different Language Models and
Generative Vision Models for
Text-to-Image Generation
Shihao Zhao1, Shaozhe Hao1, Bojia Zi2, Huaizhe Xu3, and Kwan-Yee K. Wong1
1The University of Hong Kong
shzhao,szhao,kykwongcs.hku.hk
2The Chinese University of Hong Kong
bjzise.cuhk.edu.hk
3The Hong Kong University of Science and Technology
hxubrconnect.ust.hk
Abstract. Text-to-imagegenerationhasmadesignificantadvancements
with the introduction of text-to-image diffusion models. These models
typically consist of a language model that interprets user prompts and
a vision model that generates corresponding images. As language and
vision models continue to progress in their respective domains, there is
a great potential in exploring the replacement of components in text-
to-image diffusion models with more advanced counterparts. A broader
research objective would therefore be to investigate the integration of
any two unrelated language and generative vision models for text-to-
image generation. In this paper, we explore this objective and propose
LaVi-Bridge,apipelinethatenablestheintegrationofdiversepre-trained
language models and generative vision models for text-to-image genera-
tion. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and
plug-and-play approach without requiring modifications to the original
weights of the language and vision models. Our pipeline is compatible
with various language models and generative vision models, accommo-
dating different structures. Within this framework, we demonstrate that
incorporating superior modules, such as more advanced language models
or generative vision models, results in notable improvements in capabili-
ties like text alignment or image quality. Extensive evaluations have been
conducted to verify the effectiveness of LaVi-Bridge. Code is available at
https:github.comShihaoZhaoZSHLaVi-Bridge .
Keywords: Diffusion model Text-to-image generation
1 Introduction
In recent years, there have been remarkable advancements in the field of text-to-
image generation, specifically through the use of diffusion models 9,18,43,45.
These models have made significant contributions and have gained considerable
attention for their exceptional performance. By leveraging large-scale trainingarXiv:2403.07860v1  cs.CV  12 Mar 20242 S. Zhao et al.
Encoder-OnlyLanguageModels
Encoder-DecoderLanguageModelsDecoder-OnlyLanguageModelsU-Net-BasedGenerativeVisionModelsTransformer-BasedGenerativeVisionModelsLaVi-Bridge
Fig. 1:Overview of LaVi-Bridge. LaVi-Bridge is capable of integrating various lan-
guage models and generative vision models. On the left side, we keep the vision model
fixed and experiment with different language models in our pipeline. On the right side,
we keep the language model fixed and try out different vision models. We display the
visualization results alongside each combination.
datasets alongside large deep models, text-to-image diffusion models are capable
of producing high-quality images that faithfully align with the textual descrip-
tions provided by users. This has rendered them highly applicable in real-world
scenarios such as content creation and architectural design.
Text-to-image diffusion models 2,6,29,35,38,41,51 typically consist of two
key components, namely a language model and a generative vision model. The
language model is responsible for comprehending the input prompts, whereas
the vision model is tasked with generating images that align with the extracted
context.Existingtext-to-imagediffusionmodelsemployvariouslanguagemodels
and generative vision models and have gained widespread usage. For instance,
Stable Diffusion SD 38 is a highly popular text-to-image diffusion model that
employs the CLIP text encoder 31 as its language model and a U-Net 39 as
its generative vision model. Another example is PixArt 6, a recently proposed
text-to-image diffusion model that adopts the T5 34 as its language model and
a Vision Transformer ViT 11 as its generative vision model. These models are
trained on a vast amount of text-image pairs, enabling seamless collaboration
between their language modules and vision modules.
The advancements in deep language models and deep vision models have
witnessed rapid progress in recent years, with both fields experiencing continu-
ous developments and the introduction of more powerful models. However, this
rapid development poses a challenge for the research in text-to-image gener-
ation when it comes to integrating more advanced language or vision models
into existing text-to-image diffusion models. The problem of how to integrate
any two unrelated language and vision models is unexplored, and the impact of
newly developed models on text-to-image generation capabilities also remains
uncertain. The current situation highlights the presence of a gap between the
language or vision modules within text-to-image diffusion models and the state-
of-the-art models in their respective domains. Therefore, it has become crucial
to address this gap and explore ways to incorporate more advanced language
or vision models into existing text-to-image diffusion models. Furthermore, theLaVi-Bridge 3
broader challenge of integrating any pre-trained language model with any gen-
erative vision model deserves a thorough investigation.
In this paper, our objective is to delve into the aforementioned problem. We
propose LaVi-Bridge, a flexible framework that facilitates the integration of di-
verse well-trained language models and generative vision models to achieve text-
to-image generation. Our framework enables the integration of two unrelated
language and vision models that have not been previously trained together, as
shown in Fig. 1. Importantly, LaVi-Bridge does not require modifying the orig-
inal weights of the language and vision models. Instead, it injects LoRA 20
into the language and vision models separately and utilizes an adapter to bridge
these two modules. Moreover, LaVi-Bridge only necessitates a relatively small
dataset to integrate different language models and generative vision models for
text-to-image generation.
We summarize the advantages and features of LaVi-Bridge as follows:
1. LaVi-Bridge is designed for text-to-image diffusion models and serves as
a bridge, capable of connecting various pre-trained language models and
generative vision models. Our framework can accommodate different model
structures, including encoder-only, encoder-decoder, and decoder-only lan-
guage models, as well as U-Net-based and Transformer-based generative vi-
sion models.
2. LaVi-Bridge utilizes LoRA and adapters, eliminating the need to modify
the original weights of the models. It is more flexible and requires relatively
small computing resources compared to training the entire diffusion model.
3. We evaluated various text-image alignment and image quality metrics on
shortprompts,longprompts,andcompositionalprompts.Wealsoconducted
extensive visualization. We then drew several conclusions. For instance, inte-
grating superior models leads to improved performance in the corresponding
modality, such as enhanced semantic understanding with advanced language
modelsorimprovedimagequalitywithmorepowerfulgenerativevisionmod-
els. Additionally, the diffusion model utilizing Llama-2 demonstrates excep-
tional semantic understanding, while the diffusion model utilizing the trans-
former in PixArt yields images with enhanced aesthetics.
2 Related Work
2.1 Language Models and Generative Vision Models
The mainstream Large Language Models LLMs 8,3234 are built based on
the transformer structure 49, with three main types of architectures, namely
encoder-only, encoder-decoder, and decoder-only. All these three belong to Se-
quence to Sequence Seq2Seq 47. The encoder-only architecture is exemplified
by BERT 8. CLIP text encoder 31 is based on BERT and further trained to
align with the image domain. Models of this type excel at understanding the
content of the input and generating outputs tailored to specific tasks. On the4 S. Zhao et al.
other hand, the encoder-decoder framework is adept at handling tasks that in-
volve complex mappings between input and output sequences. Examples include
T5 34 and BART 24. Recently, due to the tremendous success of ChatGPT,
attention has been drawn to models that consist solely of a decoder, like GPT-
3 4 and Llama-2 48. The decoder-only architecture demonstrates exceptional
performance in semantic understanding. For text-to-image generation, all three
types of LLMs can provide effective semantic information to serve as conditions
for image generation in diffusion models. In this paper, we explore and compare
all these three types of language models.
A generative vision model refers to a vision model with the ability to generate
images or visual contents. There are two common types of structures, namely
U-Net-based 39 and Transformer-based 11. Generative Adversarial Networks
GANs 14,37,52 employ a framework consisting of a discriminator and a
generator, with the generators structure based on U-Net. On the other hand,
motivated by the success of GPT models, recent works have attempted to use
the Transformer architecture for image generation in an autoregressive manner,
with notable examples being DALLE 36 and CogView 10. Another popular
class of generative models is diffusion models 7,18,43,45, which are based on
the diffusion process and gradually denoise to produce natural images. Early
diffusion models often employed U-Net as their generative vision model, such
as Stable Diffusion, which scaled up the Latent Diffusion Model LDM 38
with larger data scales. Some recent works have started to replace the U-Net in
diffusion models with Vision Transformer and have made significant progress,
such as DiT 30, U-ViT 1 and PixArt 6. In this paper, we focus on diffusion
models and explore both U-Net-based and Transformer-based vision models.
2.2 Text-to-Image Diffusion Models
Text-to-imagediffusionmodels2,9,29,38,41,44arecapableofgeneratingimages
based on user prompts. These models consist of two main components, namely
a language model and a vision model. The language module is responsible for
understanding the text input provided by the user, extracting contextual infor-
mation, and injecting it into the vision module to generate the desired image.
Text-to-image diffusion models have paved the way for various exciting research
areas,includingimageediting3,22,27,controllableimagegeneration28,53,54,
personalized object generation 12,16,40, as well as other interesting applica-
tions 5,13,15. In the extensive exploration of diffusion models, researchers have
utilized different language models and vision models. For instance, Stable Diffu-
sion 38 employs CLIP text encoder 31 as its language model and U-Net as its
vision model. Imagen 41 utilizes T5 34 as its language model, which claims
to enhance both sample fidelity and image-text alignment. ParaDiffusion 50
focuses on paragraph-to-image generation and leverages the powerful seman-
tic understanding capability of Llama-2 48 to comprehend lengthy sentences.
PixArt 6, on the other hand, utilizes a ViT 11 as its vision model and achieves
high image fidelity while being trained at a lower cost.LaVi-Bridge 5
After training on a large dataset of text-image pairs 42, the language and
vision models in the text-to-image diffusion model become closely intertwined.
This tight coupling ensures a strong alignment between the provided text de-
scription and the generated image, but at the same time also limits the flexibility
of the diffusion model. For instance, if a more advanced language or vision model
becomes available, it may have the potential to enhance the text-to-image task.
However, decoupling the language and vision modules in existing text-to-image
diffusion models and replacing a module with a new one is nontrivial. Therefore,
this paper explores the dilemma faced by text-to-image generation and proposes
a framework that enables efficient integration of various language models and
generative vision models.
3 Method
3.1 Preliminary
A diffusion model is based on the diffusion process for image generation. This
process consists of two stages, namely the forward process and the reverse pro-
cess. During the forward process, Gaussian noise is progressively added to a
natural image until the image becomes completely noisy. After that, during the
reverse process, the noise is gradually eliminated over a series of time steps,
resulting in a natural image. In the reverse process, a trainable vision model is
used to predict and remove the noise. By employing this denoising model, we are
able to obtain a natural image from Gaussian noise by denoising. Within a text-
to-image diffusion model, there are two components at each time step, namely a
language model fand a vision model g. The language model converts user input
textyinto embeddings, which capture the semantic meaning of the text. On
the other hand, the vision model, which is the denoising model aforementioned,
encodes image features z, extracting relevant visual information from the input
images. The interaction between text embeddings and image features is achieved
through cross-attention layers, which can be formulated as
cfy, 1
QWqz, KWkc, VWvc, 2
CrossAttention Q, K, V  softmax QKTV, 3
where Wq, WkandWvare projection matrices.
3.2 Language and Vision Alignment
LaVi-Bridge enables the integration of any two pre-trained language and gen-
erative vision models, even though these models are not related and have been
trainedseparately.Here,wedenotethelanguagemodelas fandthevisionmodel
asg, as mentioned previously. If we directly interact the textual information and6 S. Zhao et al.
GenerativeVisionModelPoolTransformer-BasedU-Net-BasedLanguageModelGenerativeVisionModelAdapterLoRALoRA
LanguageModelPoolDecoder-OnlyEncoder-DecoderEncoder-Only
FeedforwardFeedforward
Fig. 2:Pipeline of LaVi-Bridge. We select one model each from the language and
vision model pools. We then freeze the pre-trained language and vision models and
incorporate LoRA into both models. The connection between the language and vision
models is established through an adapter. The only weights we need to train are the
ones introduced by LoRA and the adapter.
image information using Eq. 1, considering that fandgare trained indepen-
dently, the parameters in the cross-attention layers of gcannot comprehend the
text embedding output by f, resulting in meaningless model outputs.
To establish a connection between them, LaVi-Bridge keeps the pre-trained
language and vision models fixed and utilizes LoRA to introduce trainable pa-
rameters into both the language model and the vision model. In this context,
we denote the language and vision models with LoRA as f11andg22,
where 1and2are the original parameters of fandg, respectively. Further-
more,weintroduceanadapterasabridgebetweenthelanguagemodelandvision
model to facilitate better alignment. The adapter consists of stacked feedforward
layers, denoted as h. Consequently, the cross-attention layer can be expressed as
cf11y, 4
QW22
q z, KWk22hc, VW22
v hc,5
CrossAttention Q, K,V  softmax QKTV. 6
Now, we only need to train 1,2, and hon a relatively small amount of
text-image pairs. After training, the language and generative vision models can
effectively collaborate to generate meaningful images. We present the framework
of LaVi-Bridge in Fig. 2. LaVi-Bridge is very straightforward, with both LoRA
and the adapter being its crucial and indispensable components.
3.3 Design Details
LaVi-Bridge is designed to accommodate a wide range of language model struc-
tures, including encoder-only, encoder-decoder, decoder-only, as well as genera-
tive vision model structures such as U-Net and ViT. In the language model,
we inject LoRA into all linear layers of the attention layers. Likewise, in aLaVi-Bridge 7
transformer-based vision model, LoRA is injected into all linear layers of the
attention layers. In a U-Net-based vision model, LoRA is injected into all linear
layers and convolutional layers of the ResBlocks, attention, and cross-attention
layers. To address the dimension disparity between the output embedding of the
language model and the dimensions handled by the cross-attention of the vision
model, we employ two feedforward layers for the adapter. The input dimension
of the adapter matches the output text embedding dimension of the language
model, while the output dimension aligns with the dimensions received by the
cross-attention of the vision model.
For training, we first select the language and generative vision models that
we choose to integrate. We keep their original weights fixed and train LoRA
and the adapter on text-image pairs following the design mentioned above. The
trainedLoRAandadapterhavefewerparameterscomparedtotheoriginalmodel
weights, which makes LaVi-Bridge highly flexible. For evaluation, we used var-
ious metrics to assess text alignment and image quality across short prompts,
long prompts, and compositional prompts.
4 Experiments
4.1 Experimental Settings
In this section, we explored the performance of different language models and
generative vision models under LaVi-Bridge. We also tested the impact of LoRA
andadapters.Wetrainedonadatasetconsistingofatotalof1milliontext-image
pairs, including around 600k text-image pairs from the COCO2017 25 train set
and 400k text-image pairs from an internal dataset with high-quality images
and captions. For each setting, we set the LoRA rank to 32, image resolution to
512512and the batch size to 256. We used the AdamW optimizer 26 with a
learning rate of 1104and trained for a total of 50ksteps. During inference,
we employed the DDIM sampler 43 for sampling with the number of time steps
set to 50and the classifier free guidance scale 19 set to 7.5.
As mentioned above, we conducted our quantitative evaluation on short
prompts, long prompts, and compositional prompts. Specifically,
1. Forshortprompts,weevaluatedusingtheCOCO201425validationset.We
randomly sampled 30k images and tested image quality and text alignment
within this subset. We used FID 17 and aesthetic score 23 as evaluation
metrics for image quality and CLIP score for text alignment.
2. For long prompts, we employed the same 30k-subset of COCO2014 and uti-
lized Llama-2 to generate expanded captions ranging from 20to70words to
construct a dataset of 30k long prompts. Since the caption expansion process
does not refer to the content of the reference image, we solely used aesthetic
score to evaluate image quality and CLIP score for text alignment.
3. For compositional prompts, we utilized the benchmark proposed by Comp-
bench 21. Compositional prompts were mainly used to test the models8 S. Zhao et al.
CLIPT5-SmallT5-BaseT5-LargeLlama-2
Marvel movie character, iron man, dress up to match movie character, full body photo, American apartment, lying down, life in distress, messy, lost hope, food, wine, hd, 8k, real, reality, super detail, 8k post photo manipulation, real photo.Digital illustration of a beach scene crafted from yarn. The sandy beach is depicted with beige yarn, waves are made of blue and white yarn crashing onto the shore. A yarn sun sets on the horizon, casting a warm glow. Yarn palm trees sway gently, and little yarn seashells dot the shoreline.Acute cat.An illustration of a human heart made of translucent glass, standing on a pedestal amidst a stormy sea. Rays of sunlight pierce the clouds, illuminating the heart, revealing a tiny universe within.
A paper craft art depicting a girl giving her cat a gentle hug. Both sit amidst potted plants, with the cat purring contentedly while the girl smiles. The scene is adorned with handcrafted paper flowers and leaves.A middle-aged woman of Asian descent, her dark hair streaked with silver, appears fractured and splintered, intricately embedded within a sea of broken porcelain. The porcelain glistens with splatter paint patterns in a harmonious blend of glossy and matte blues, greens, oranges, and reds, capturing her dance in a surreal juxtaposition of movement and stillness. Her skin tone, a light hue like the porcelain, adds an almost mystical quality to her form.
An ancient stone Colossus with eye, Stephan Martiniere, dark yellow and light emerald, color zone painting, Denis Sarazhin, dark emerald and silver, robotic expressionism, high detail.
London luxurious interior living-room.
Fig. 3:Visualization results of LaVi-Bridge with different language models. The first
row to the fifth row present the results with CLIP text encoder, T5-Small, T5-Base,
T5-Large, and Llama-2, respectively. The prompts are displayed at the top or bottom
of each column.
understanding of textual attributes, such as generating correct object prop-
erties like color and shape, as well as accurate relationships between objects,
such as spatial positioning.
We conducted a user study on different combinations of language and vision
models. For each combination, we evaluated two metrics, namely image quality
and text alignment. Users were asked to rank the generated images based on
these evaluation criteria. The image ranked last received a score of 1, the second-
to-last received a score of 2, and so on. We then calculated the percentage of
scoresforeachmodel.Weselected 20promptsandincluded 30usersparticipated
in the testing. In addition to quantitative evaluation and user study, we provided
amplevisualizationresultsineachsectiontoofferamoreintuitiveunderstanding
of the performance of each model.
4.2 Evaluation on Different Language Models
This section evaluates the performance of LaVi-Bridge with different language
models. We fixed the vision model to the U-Net of Stable Diffusion V1.4 and
integrated it with different language models under LaVi-Bridge. We considered
CLIP text encoder, based on the encoder-only framework, T5 series T5-Small,
T5-Base, T5-Large, based on the encoder-decoder framework, and Llama-2-7B,LaVi-Bridge 9
Table 1: Quantitative evaluation of LaVi-Bridge with different language models.
Short, Long and Comp denote short prompts, long prompts, and compositional
prompts respectively. The best results are in bold.
CLIP T5-Small T5-Base T5-Large Llama-2
Short - FID 23.57 22.98 22.62 23.11 21.80
Short - Aesthetics 5.609 5.813 5.888 5.881 5.883
Short - CLIP Score 0.3102 0.3122 0.3149 0.3156 0.3172
Long - Aesthetics 6.003 6.206 6.284 6.305 6.355
Long - CLIP Score 0.3120 0.3111 0.3179 0.3193 0.3231
Comp - Color 0.3578 0.3368 0.3856 0.3889 0.4859
Comp - Shape 0.3752 0.2962 0.3266 0.3552 0.4285
Comp - Texture 0.4506 0.3728 0.4132 0.4524 0.5055
Comp - Spatial 0.1296 0.1456 0.1569 0.1582 0.1914
Comp - Non-Spatial 0.3009 0.2984 0.3054 0.3068 0.3106
Comp - Complex 0.2985 0.2728 0.3055 0.3072 0.3094
based on the decoder-only framework. We present the visualization results in
Fig. 3, quantitative evaluation in Tab. 1, and user study in Fig. 5.
Visualization From Fig. 3, we can observe that with LaVi-Bridge, all these
language models can effectively integrate with U-Net of Stable Diffusion V1.4
and generate meaningful results, such as cases of the cat and living room in
Fig. 3. This demonstrates the great generalization ability of LaVi-Bridge for
various language models. Additionally, we notice that the performance of dif-
ferent model structures varies when the provided prompts contain more com-
plex semantics. We find that the text-to-image diffusion model corresponding to
Llama-2 can perfectly describe semantic information. For example, in the third
column, Llama-2s generated result effectively integrates a woman into the sea
of fragmented porcelain. In the fourth column, it correctly understands and gen-
erates both the girl and the cat in a paper craft art. In the seventh column,
it even portrays an entire beach scene using yarn. These examples surpass the
capabilities of those models with CLIP and T5. Furthermore, we observe that
T5-Large and Llama-2 accurately generate food and wine in the case of Iron
Man, and in the last column, they successfully generate an ancient stone with
eyes in dark yellow and emerald. Models with CLIP text encoder, T5-Small,
and T5-Base are not able to capture these cases accurately.
Quantitative Evaluation From Tab. 1, we can observe that Llama-2 achieves
the best results for all the metrics used to evaluate text alignment ability, under
the setting of all the short prompts, long prompts, and compositional prompts.
Besides, Llama-2 also performs the best on most of the metrics evaluating image
quality. On the other hand, as the model capacity increases, in general circum-
stances T5-Large usually outperforms T5-Base, and T5-Base outperforms T5-
Small in the area of Natural Language Processing. This conclusion also holds
true for LaVi-Bridge. For all the metrics used to evaluate text alignment ability
in Tab. 1, T5-Large is superior to T5-Base, and T5-Base is superior to T5-Small.
This tells us that incorporating a better language model into the text-to-image10 S. Zhao et al.
U-NetLDMU-NetSDTransformerPixArt
Abandoned city with ruined buildings, long deserted streets, cars aged by time, trees, flowers, scattered leaves, empty street, vibrant colors, lineart.A fierce garden gnome warrior, clad in armor crafted from leaves and bark, brandishes a tiny sword and shield. He stands valiantly on a rock amidst a blooming garden, surrounded by colorful flowers and towering plants. A determined expression is painted on his face, ready to defend his garden kingdom.Forest.
Smooth meat table, restaurant, Paris, elegant, lights.Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.
A swirling, multicolored portal emerges from the depths of an ocean of coffee, with waves of the rich liquid gently rippling outward. The portal engulfs a coffee cup, which serves as a gateway to a fantastical dimension. The surrounding digital art landscape reflects the colors of the portal, creating an alluring scene of endless possibilities.A natural landscape painting with white clouds floating in the blue sky. There are several mountains below with some plants growing on the mountains. There is a sea below the mountains. There is a house made of stone and wood on the shore. There are many green plants next to the house.Portrait photography, a woman in a glamorous makeup, wearing a mask with tassels, in the style of midsommar by Ari Aster, made of flowers, bright pastel colors, prime lense.
Fig. 4:Visualization results of LaVi-Bridge under different generative vision models.
The first row to the third row present the results with U-Net in Latent Diffusion Model,
U-Net in Stable Diffusion V1.4 and transformer in PixArt, respectively. The prompts
are displayed at the top or bottom of each column.
diffusion model under LaVi-Bridge can lead to improved text alignment. This
makes one of the motivations of LaVi-Bridge meaningful, which is that replacing
the model in the existing text-to-image diffusion model with a better model can
lead to performance improvements.
User Study We follow the settings described in Sec. 4.1, and the results are
presented in the two disk diagrams on the left side of Fig. 5. The model using
Llama-2 demonstrates the best performance in terms of both image quality and
text alignment, with a particularly pronounced advantage in text alignment. On
the other hand, CLIP and T5-Small exhibit noticeably poorer performance on
both image quality and text alignment compared to other models.
4.3 Evaluation on Different Vision Models
Thissection evaluatesthe performanceofLaVi-Bridgewith differentvisionmod-
els. We fixed the language model to T5-Large and integrated it with different
generative vision models under LaVi-Bridge. We considered the well-trained U-
Nets in the Latent Diffusion Model and Stable Diffusion V1.4, as well as the
Vision Transformer in PixArt, totally three models. We present the visualiza-
tion results in Fig. 4, quantitative evaluation in Tab. 2, and user study in Fig. 5.
Visualization From Fig. 4, we can see that all these three vision models inte-
grate well with T5-Large and generate relatively accurate images based on the
given text prompts. From these cases, we can observe that the images generated
by the transformer model based on PixArt exhibit richer details compared to
the images generated by the other two models based on U-Net. For example,
the forest in the first column, the hull of the pirate ship in the third column,
and the bushes at the foot of the mountain in the sixth column are very intri-
cate and realistic. Additionally, we can observe from these cases that the images
generated by the model with U-Net of Stable Diffusion V1.4 have more detailedLaVi-Bridge 11
Table 2: Quantitative evaluation of LaVi-Bridge under different generative vision
models. Short, Long and Comp denote short prompts, long prompts, and compo-
sitional prompts respectively. The best results are in bold.
U-NetLDM U-NetSD TransformerPixArt
Short - FID 25.94 23.11 23.02
Short - Aesthetics 5.703 5.881 6.145
Short - CLIP Score 0.3126 0.3156 0.3172
Long - Aesthetics 6.122 6.305 6.406
Long - CLIP Score 0.3189 0.3193 0.3210
Comp - Color 0.4099 0.3889 0.3689
Comp - Shape 0.3724 0.3552 0.3316
Comp - Texture 0.5046 0.4524 0.4553
Comp - Spatial 0.1550 0.1582 0.1725
Comp - Non-Spatial 0.3004 0.3068 0.3098
Comp - Complex 0.3060 0.3072 0.3014
features compared to the images generated by the model with U-Net of Latent
DiffusionModel.Furthermore,wecanalsofindthat,forthePixArt-basedmodel,
text alignment is better in some cases. For instance, in the image of the fifth
column, only the model that is based on the transformer of PixArt generates
the aged car mentioned in the prompt. Similarly, in the seventh column, the
garden warrior holding a sword and shield is highly consistent with the prompt
description.
Quantitative Evaluation From Tab. 2, it can be observed that for all the met-
rics measuring image quality, LaVi-Bridge with the PixArt vision model achieves
the best results. Additionally, PixArt also achieves the best text alignment for
both short and long prompts. This reflects the use of PixArts transformer as a
vision model can also improves the models understanding of semantics to some
extent. Additionally, it is noteworthy that the U-Net in Stable Diffusion, an
enhanced version of the U-Net utilized in the Latent Diffusion Model, still out-
performs Latent Diffusion Models U-Net under LaVi-Bridge on all the metrics
measuring image quality. This aligns with our previous discussion in Sec. 4.2 and
further validates the underlying motivation behind our proposed LaVi-Bridge.
User Study We follow the settings described in Sec. 4.1, and the results are
presented in the two disk diagrams on the right side of Fig. 5. The model using
thetransformerfromPixArtdemonstratesthebestperformanceintermsofboth
image quality and text alignment, with a notably significant advantage in image
quality. Additionally, the U-Net in Stable Diffusion outperforms the U-Net in
the Latent Diffusion Model overall.
4.4 Ablation Study
In this section, we investigate two sets of ablation experiments. The first set aims
to explore the impact of training LaVi-Bridge on the original pre-trained text-
to-image diffusion model. The second set of experiments is to study the effects12 S. Zhao et al.
ImageQualityTextAlignmentImageQualityTextAlignment
CLIPT5-SmallT5-BaseT5-LargeLlama-2
U-NetLDMU-NetSDTransformerPixArt24212218152722201615423424383428
Fig. 5:User study. The two disk diagrams on the left display the users scoring results
on different language models, while the two disk diagrams on the right display the
users scoring results on different generative vision models. The percentage represents
the proportion of the score obtained by a model out of the total score of all models.
of LoRA and adapters in LaVi-Bridge. For both sets of experiments, we present
visualization results in Fig. 6 and provide quantitative evaluations in Tab. 3.
Training with LaVi-Bridge We investigate the impact of our LaVi-Bridge
training framework on the original pre-trained text-to-image diffusion model.
Specifically, we consider Stable Diffusion V1.4 which adopts CLIP text encoder
as its language model and U-Net as its vision model. We incorporate LoRA
and an adapter and apply LaVi-Bridge to the same language and vision models
with identical structures and weights to those in Stable Diffusion V1.4. We then
compare the performance of the model under LaVi-Bridge with the original
Stable Diffusion V1.4.
The visualization results are shown in the first two rows of Fig. 6. For these
two models, there is no significant difference in image quality and text align-
ment, varying on a case-by-case basis. In some cases, Stable Diffusion performs
better, such as in the third column, where Stable Diffusion successfully generates
the case of a Fox bracelet made of buckskin with fox features, while the model
trained under LaVi-Bridge only generates the fox and fails to understand the
bracelet made of buckskin. Similarly, in the case of Marvels Hulk playing basket-
ball,StableDiffusiongeneratesaslamdunkactionfollowingtheprompt,whereas
the model trained under LaVi-Bridge does not. However, in the second column,
the model trained under LaVi-Bridge correctly understands the quantity and
successfully generates two elephants, while Stable Diffusion only generates one.
Moreover, in the last column, the model trained under LaVi-Bridge accurately
describes a frog in a spacesuit, while Stable Diffusion fails.
The left two columns of Tab. 3 present the quantitative evaluation results. It
can be observed that Stable Diffusion achieves the best image quality and text
alignment for both short prompts and long prompts. However, for compositional
prompts, the model trained under LaVi-Bridge outperforms Stable Diffusion in
four out of six settings.
Based on the visualization results and quantitative evaluations, we can con-
clude that overall there is no significant improvement or decline in text align-
ment. Regarding image quality, it should be noted that training with LaVi-
Bridge may result in a decrease compared to the original text-to-image diffusion
model, if the same models and weights are used. However, it is important toLaVi-Bridge 13
SDCLIPU-NetT5U-NetwoAdapterwoLoRA
A mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by colorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters through a nearby window.A rabbit, forest in spring.A cute frog wearing a space suit floating through space but sleepy.White Nelore bull, anthropomorphic adult cattle, a Weightlifter, very strong, doing a lot of strength, expression of strength, lifting a very heavy barbell, weightlifter position, doing Weightlifting, wearing athlete's clothes, action, white background, full body, surreal realistic.Marvels Hulk playing basketball, he is jumping to dunk on the hoop, and he is dressed in the Lakers uniform, realistic perspective, graphics, surrealistic realism, emotive realism.
Two elephants in the forest.Fox bracelet made of buckskin with fox features.
Sea.
Fig. 6:Visualization results of the ablation study. The top two rows show the impact
of LaVi-Bridge on the original pre-trained text-to-image diffusion models. The bottom
three rows illustrate the influence of the adapter and LoRA. The prompts are displayed
at the top or bottom of each column.
understand that the main purpose of LaVi-Bridge is to establish connections
between different language and vision models, enabling the utilization of more
advanced models for performance enhancement. It is not intended to be directly
applied to the original text-to-image diffusion models using the same models and
weights.
LoRA and Adapter Here, we investigate the role of LoRA and adapters in
LaVi-Bridge.WeuseT5-LargeasthelanguagemodelandStableDiffusionV1.4s
U-Net as the vision model. For the LoRA experiments, we kept the language and
visionmodelsfixedwithoutintroducingLoRA,and onlytrainedtheadapter.For
the adapter experiments, considering the mismatch in the dimensions of text
embeddings from the language model and the input embeddings acceptable by
the vision model, we aligned the dimensions between the language and vision
models using a single linear layer instead of stacked feedforward layers which
include non-linear activation layers. Under this setting, we trained both LoRA
and this linear layer.
The visualization results are shown in the bottom three rows of Fig. 6. We
can observe that both image quality and text alignment are significantly affected
when LoRA and adapters are not used. For example, in the case of Bull Fit
Athlete in the seventh column, without LoRA or the adapter, the model cannot
understand and integrate these two less related elements, and the image quality
is much lower compared to results generated by the original setting. We also14 S. Zhao et al.
Table 3: Quantitative evaluation of the ablation study. The left two columns present
the impact of LaVi-Bridge on the original pre-trained text-to-image diffusion models.
The right three columns demonstrate the influence of the adapter and LoRA. Short,
Long and Comp denote short prompts, long prompts, and compositional prompts
respectively. The best results are in bold.
SD CLIPU-Net wo Adapter wo LoRA T5U-Net
Short - FID 20.32 23.57 23.81 22.35 23.11
Short - Aesthetics 5.899 5.609 5.807 5.829 5.881
Short - CLIP Score 0.3132 0.3102 0.3147 0.3107 0.3156
Long - Aesthetics 6.120 6.003 6.131 6.273 6.305
Long - CLIP Score 0.3171 0.3120 0.3106 0.3097 0.3193
Comp - Color 0.3570 0.3578 0.3550 0.2485 0.3889
Comp - Shape 0.3563 0.3752 0.3044 0.2944 0.3552
Comp - Texture 0.4028 0.4506 0.4001 0.3190 0.4524
Comp - Spatial 0.1225 0.1296 0.1651 0.0956 0.1582
Comp - Non-Spatial 0.3104 0.3009 0.3065 0.2998 0.3068
Comp - Complex 0.3042 0.2985 0.2878 0.2687 0.3072
found that the results without LoRA are worse than those without the adapter.
For instance in the fourth column, there is not even a ferret present in the image
in the absence of LoRA.
The right three columns of Tab. 3 present the quantitative evaluation results.
We find that our default setting, which utilizes both LoRA and the adapter,
achieves the best performance in most cases. Additionally, overall, the absence
of LoRA has a significant impact on text alignment, with many text alignment
evaluation metrics being much lower compared to the absence of the adapter.
5 Conclusion
In this paper, we propose LaVi-Bridge, which works on text-to-image diffusion
models. LaVi-Bridge is capable of connecting various language models and gen-
erative vision models for text-to-image generation. It is highly versatile and can
adapt to different structures. LaVi-Bridge is also flexible, as it achieves inte-
gration without modifying the original weights of language and vision models.
Instead, it utilizes LoRA and an adapter for fine-tuning. Additionally, under
LaVi-Bridge,usingsuperiorlanguageorvisionmodelscanenhancethetextcom-
prehension capability or image quality. These advantages enable LaVi-Bridge to
helptext-to-imagediffusionmodelsleveragethe latestadvancementsinthe areas
of Natural Language Processing and Computer Vision, to enhance text-to-image
generation.Webelievethatthistaskholdssignificantresearchvalueandrequires
further exploration. LaVi-Bridge allows designers, artists, and others to flexibly
utilize existing language and vision models to achieve their creative goals. It
is of utmost importance to avoid misuse and mitigate potential negative social
impacts. In practical deployment, it is crucial to standardize its usage, improve
model transparency.LaVi-Bridge 15
References
1. Bao,F.,Li,C.,Cao,Y.,Zhu,J.:Allareworthwords:avitbackboneforscore-based
diffusion models. CVPR 2023 4
2. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang,
J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., Ramesh,
A.: Improving image generation with better captions. https:cdn.openai.com
papersdall-e-3.pdf 2023 2, 4
3. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. CVPR 2023 4
4. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. NeurIPS 2020 4
5. Chen, J., Huang, Y., Lv, T., Cui, L., Chen, Q., Wei, F.: Textdiffuser: Diffusion
models as text painters. NeurIPS 2024 4
6. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P.,
Lu, H., et al.: Pixart- : Fast training of diffusion transformer for photorealistic
text-to-image synthesis. ICLR 2024 2, 4
7. Croitoru, F.A., Hondru, V., Ionescu, R.T., Shah, M.: Diffusion models in vision:
A survey. TPAMI 2023 4
8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectional transformers for language understanding. NAACL-HLT 2018 3
9. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS
2021 1, 4
10. Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao,
Z., Yang, H., et al.: Cogview: Mastering text-to-image generation via transformers.
NeurIPS 2021 4
11. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale. ICLR
2021 2, 4
12. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
Cohen-Or, D.: An image is worth one word: Personalizing text-to-image gener-
ation using textual inversion. arXiv preprint arXiv:2208.01618 2022 4
13. Ge, S., Park, T., Zhu, J.Y., Huang, J.B.: Expressive text-to-image generation with
rich text. ICCV 2023 4
14. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS 2014 4
15. Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D.,
Dai, B.: Animatediff: Animate your personalized text-to-image diffusion models
without specific tuning. ICLR 2024 4
16. Hao, S., Han, K., Zhao, S., Wong, K.Y.K.: Vico: Detail-preserving visual condition
for personalized text-to-image generation. arXiv preprint arXiv:2306.00971 2023
4
17. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
by a two time-scale update rule converge to a local nash equilibrium. NeurIPS
2017 7
18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS
2020 1, 416 S. Zhao et al.
19. Ho, J., Salimans, T.: Classifier-free diffusion guidance. NeurIPS Workshop on Deep
Generative Models and Downstream Applications 2021 7
20. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,
W.: Lora: Low-rank adaptation of large language models. ICLR 2022 3
21. Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.: T2i-compbench: A comprehensive
benchmark for open-world compositional text-to-image generation. arXiv preprint
arXiv: 2307.06350 2023 7
22. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.: Imagic: Text-based real image editing with diffusion models. CVPR 2023 4
23. LAION-AI: aesthetic-predictor. https:github.comLAION- AIaesthetic-
predictor 2022 7
24. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
Stoyanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461 2019 4
25. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. ECCV 2014 7, 20
26. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. ICLR 2019 7
27. Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: Sdedit: Guided
image synthesis and editing with stochastic differential equations. ICLR 2022 4
28. Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter:
Learning adapters to dig out more controllable ability for text-to-image diffusion
models. arXiv preprint arXiv:2302.08453 2023 4
29. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
Sutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting
with text-guided diffusion models. ICML 2021 2, 4
30. Peebles, W., Xie, S.: Scalable diffusion models with transformers. ICCV 2023 4
31. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. ICML 2021 2, 3, 4
32. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-
guage understanding by generative pre-training. OpenAI blog 2018 3
33. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI blog 2019 3
34. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,
W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
transformer. JMLR 2020 2, 3, 4, 21
35. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
2022 2
36. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
Sutskever, I.: Zero-shot text-to-image generation. ICML 2021 4
37. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative
adversarial text to image synthesis. ICML 2016 4
38. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. CVPR 2022 2, 4, 21
39. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-
ical image segmentation. MICCAI 2015 2, 4
40. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation.
CVPR 2023 4LaVi-Bridge 17
41. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. NeurIPS 2022 2,
4
42. Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A.,
Coombes,T.,Jitsev,J.,Komatsuzaki,A.:Laion-400m:Opendatasetofclip-filtered
400 million image-text pairs. arXiv preprint arXiv:2111.02114 2021 5
43. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. ICLR 2021
1, 4, 7
44. Song, Y., Dhariwal, P., Chen, M., Sutskever, I.: Consistency models. ICML 2023
4
45. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
based generative modeling through stochastic differential equations. ICLR 2021
1, 4
46. Sun, K., Pan, J., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z.,
Wang, Y., et al.: Journeydb: A benchmark for generative image understanding.
NeurIPS 2024 21
47. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural
networks. NeurIPS 2014 3
48. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 2023 4
49. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
., Polosukhin, I.: Attention is all you need. NeurIPS 2017 3
50. Wu, W., Li, Z., He, Y., Shou, M.Z., Shen, C., Cheng, L., Li, Y., Gao, T., Zhang,
D., Wang, Z.: Paragraph-to-image generation with information-enriched diffusion
model. arXiv preprint arXiv:2311.14284 2023 4
51. Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., Luo, P.: Raphael: Text-to-
image generation via large mixture of diffusion paths. NeurIPS 2024 2
52. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.N.: Stack-
gan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. ICCV 2017 4
53. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. ICCV 2023 4
54. Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-
controlnet: All-in-one control to text-to-image diffusion models. NeurIPS 2024
418 S. Zhao et al.
A Long Prompts
In this section, we provided a comprehensive explanation of the evaluation for
long prompts mentioned in the main paper. To conduct this evaluation, we uti-
lized the subset of 30k text-image pairs from COCO2014, which was originally
used for evaluating short prompts, and employed the Llama-2 to extend the
captions within this subset to a length of 20-70 words.
Fig. 7:Statistics regarding the long prompts utilized in evaluations in the main paper.
The figure on the left visualizes word frequency. The histogram on the right presents
the distribution of sentence length, where horizontal axis represents the range of word
counts in the prompts, and the vertical axis represents the number of prompts falling
within each sentence length range.
Specifically, we used the Llama-2-7b-chat model and provided the following
prompt:Please expand the caption to 20-70 words to enrich its semantic mean-
ing:  placeholder . Just give me one answer. Here, the term  placeholder 
represents the short prompts that we aimed to expand. In this way, we success-
fully generated 30k long prompts for evaluation. Here are a few examples of the
generated long prompts:
1. Short Prompt: a brown, white and yellow bird standing in the grass. 
Long Prompt: A striking, multi-hued bird with warm brown plumage, crisp
white patches on its wings, and vibrant yellow feathers stands gracefully in
lush green grass, creating a picturesque scene.
2. Short Prompt: two sheep standing in the snow with one looking for food
Long Prompt: Two fluffy white sheep stand stately in the pristine snow,
their wool glistening under the crisp sunlight. One of them eagerly scans the
ground, sniffing out potential nourishment amidst the frozen landscape, while
the other stands watchful and still, seemingly lost in thought.
3.
We presented statistical analysis of the generated long prompts in the Fig. 7.
The figure on the left visualizes word frequency, while histogram on the right
illustrates the distribution of sentence lengths. We observed that the vocabulary
diversityinlongpromptsisquiterich,andthemajorityoflongpromptstypically
have sentence lengths ranging from 20 to 40 words, and there is also a certain
portion that falls within the 40-60 words range.LaVi-Bridge 19
B Training Cost
As mentioned in the main paper, LaVi-Bridge does not require modifying the
original weights of the language and vision models. Instead, it introduces and
trains LoRA and adapters. This approach significantly reduces the training cost
comparedtotrainingtheentiretext-to-imagediffusionmodel.Forthetrainingof
LaVi-Bridge, we utilized 8 A100 GPUs with a batch size of 256and completed
the training in less than 2 days. Furthermore, we provided a comparison of
the number of parameters for different language and vision model combinations
in Tab. 4. The leftmost column shows the language and vision models used.
It can be observed that training only the LoRA and the adapter leads to a
significant reduction in the number of trainable parameters compared to training
theoriginallanguageandvisionmodel.Additionally,bothLoRAandtheadapter
are plug-and-play components, making LaVi-Bridge highly flexible.
Table 4: Comparison of number of parameters.
Language VisionSumAdapterLanguage VisionSumModel Model LoRA LoRA
CLIP123M 860M 983M 14M 2M 28M 44MU-NetSD
T5-Small35M 860M 895M 9M 0.8M 28M 38MU-NetSD
T5-Base110M 860M 970M 14M 2M 28M 44MU-NetSD
T5-Large335M 860M 1195M 21M 6M 28M 55MU-NetSD
Llama-26738M 860M 7598M 229M 34M 28M 291MU-NetSD
T5-Large335M 872M 1207M 30M 6M 29M 65MU-NetLDM
T5-Large
335M 611M 946M 113M 6M 17M 136M Transformer
PixArt
C Training Steps
As mentioned in the main paper, we trained the LaVi-Bridge for 50k steps. In
this section, we present the generated images as training progresses. We utilized
the T5-Large as the language model and the U-Net from Stable Diffusion V1.4
as the vision model.
Fig. 8 illustrates the evolution of the models performance. Initially, during
the first 1k steps, the image quality was poor, and the model struggled to com-
prehend the given prompt. However, as the training progressed to 10k steps,20 S. Zhao et al.
there was a significant improvement in image quality. By the time it reached 20k
steps, the model exhibited enhanced semantic understanding. Finally, at 50k
steps, the model demonstrated further optimization compared to the model at
20k step, showcasing the best performance.
TrainingSteps
50k1k5k10k20kA mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by colorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters through a nearby window.
Fig. 8:The results for different training steps. Prompts are displayed above, while the
number of training steps is shown below.
There is a fire hydrant on the street while a van is in the background.COCO2017InternalDataset
A cat sleeps on top of a computer mouse.
A baseball player on the backswing of hitting a pitch.
a big jumbo jet sits parked at an airport
some food is in a glass jar in a blender
three men around a table looking at an object.
stock image popular. Fried eggs trend.
nordic lake with a small wood house, evening, polarlights, bright colors, hyper realistic, lots of details, resolution 8k
An expansive, ultramodern living room serves as the backdrop for this empty horizontal portrait frame mockup. The frame is set against a minimalist concrete wall, surrounded by contemporary furnishings and sleek floortoceiling windows. Plush, neutraltoned seating and stylish accessories create an inviting atmosphere, with the frame remaining the focal point, free of any distractions or obstructions.
artistic award winningwide photograph of congressional chambers inside of a grand ornate cathedral that survived the apocalypse, national geographic photo, rubble on floor, night time, octane render, war photography, dark cinematic color grade
oil painting of clouds of colors on center of white background touching no borders ,with full colors,nice transition,pink color, soft details
oil painting, a blonde warrior prince in green armor stepping through the rubble of a falling palace, dawn, storming, dramatic, fantasy, medevial, ultra realistic
Fig. 9:Examples from COCO2017 train set and the internal dataset.
D Training Set
As mentioned in our main paper, we conducted training using the COCO2017
25 train set, which consists of around 600k text-image pairs, along with an ad-
ditional 400k internal data. The COCO2017 dataset primarily comprises highly
realistic images with short and straightforward captions. In order to enhance the
diversity and quality of the training data, we collected an additional 400k text-
image pairs that exhibit a wide range of artistic styles and high-quality visuals,
accompanied by accurate and detailed captions. We provide some examples of
the COCO2017 dataset and the internal dataset in Fig. 9.LaVi-Bridge 21
COCO2017COCO2017InternalDatasetThe sleek black cat sat next to the fluffy white pillow
Two elephants in the forest.A brown bench sits in front of an old white building
The soft, warm glow of the candle created a cozy ambiance, a gentle flicker of light in the darknessA swirling, multicolored portal emerges from the depths of an ocean of coffee, with waves of the rich liquid gently rippling outward. The portal engulfs a coffee cup, which serves as a gateway to a fantastical dimension. The surrounding digital art landscape reflects the colors of the portal, creating an alluring scene of endless possibilities
An icy landscape, where a magnificent frozen waterfall flows over a cliff. In the center of the scene, a fire burns bright, its flames seemingly frozen in place, casting a shimmering glow on the surrounding ice and snow.
Ancient pages filled with sketches and writings of plants and monsters sprawl across an old, weathered journal. The faded dark green ink tells tales of magical adventures, while the high-resolution drawings detail each creatures intricate characteristics.A mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by colorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters through a nearby window
Fig. 10: Comparison of training only on COCO2017 and training on both COCO2017
and the internal dataset.
In Fig. 10, we present a comparison of the results obtained from training on
COCO2017 alone and training on both COCO2017 and our internal dataset. We
used T5-Large 34 as the language model and Stable Diffusion V1.4s U-Net 38
as the vision model for LaVi-Bridge. It can be observed in Fig. 10 that in the first
three columns, the model trained solely on COCO2017 performs well in terms
of both image quality and text alignment. The model accurately understands
quantities, as seen in the case of two elephants, and attributes, such as the black
cat and white pillow or the brown bench in front of the white building, and
so on. Furthermore, the model is capable of generating images using complex
prompts, as demonstrated in columns 4-6. However, when the model is tasked
with generating images depicting fanciful scenarios, as shown in the seventh
column, or images with a non-realistic style, as shown in the last column, the
model trained only on COCO2017 struggles to produce such images.
As mentioned earlier, the text-image pairs in COCO2017, from a certain per-
spective, lack diversity. This is because the images in COCO2017 have a highly
realistic style, resulting in a lack of variety, and their quality is relatively low.
Additionally, all the captions in COCO2017 are short and very direct, as illus-
trated in Fig. 9. Consequently, it is expected that models trained on COCO2017
will face challenges in generating images of whimsical scenarios or non-realistic
styles. Fortunately, there are now many high-quality text-image datasets avail-
able, such as 46, and we highly recommend incorporating these datasets into
training in order to achieve better image generation.
Here, we want to emphasize the contribution of LaVi-Bridge again. LaVi-
Bridge is a framework designed to bridge various language and vision models.
Even though it was trained only on COCO2017, as can be seen from the first
six columns in Fig. 10, LaVi-Bridge is still effective. The reason for the poor
performanceinthelasttwocolumnsinthefirstrowofFig.10isduetothelimited
diversity of the COCO2017 training set as previously mentioned. If LaVi-Bridge
is trained on a more general text-image dataset, it will become more versatile,
enabling better generation on a wide variety of images.22 S. Zhao et al.
E More Visualization Results
In this section, we present additional visualization results in Fig. 11. In columns
one through five, we kept the vision model fixed as U-Net from Stable Diffusion
V1.4andemployeddifferentlanguagemodels.Incolumnsfour,six,andseven,we
kept the language model fixed as T5-Large and utilized different vision models.LaVi-Bridge 23
CLIPU-Net-SDT5-SmallU-Net-SDT5-BaseU-Net-SDT5-LargeU-Net-SDLlama-2U-Net-SDT5-LargeU-Net-LDMT5-LargeTransformer-PixArt32 bit pixelated future Hiphop producer in glowing power street ware, noriyoshi ohrai, in the style of minecraft tomer hanuka.Dark high contrast render of a psychedelic tree of life illuminating dust in a mystical cave.Oppenheimer sits on the beach on a chair, watching a nuclear exposition with a huge mushroom cloud, 120mm.Hyper real glass flowers, blue organic twisting. octane render, unreal engine, blender render, immersive detail, enhanced quality.Tiny potato kings wearing majestic crowns, sitting on thrones, overseeing their vast potato kingdom filled with potato subjects and potato castles.Human life depicted entirely out of fractals.
A stylized portrait-oriented depiction where a tiger serves as the dividing line between two contrasting worlds. To the left, fiery reds and oranges dominate as flames consume trees. To the right, a rejuvenated forest flourishes with fresh green foliage. The tiger, depicted with exaggerated and artistic features, stands tall and undeterred, symbolizing natures enduring spirit amidst chaos and rebirth.
A 3D render of a coffee mug placed on a window sillduring a stormy day. The storm outside the window is reflected in the coffee, with miniature lightning bolts and turbulent waves seen inside the mug. The room is dimly lit, adding to the dramatic atmosphere.
Mountains.
Outdoor style fashion photo, full body shot of a man with short brown hair, happy and smiling, he is standing on his hipster bicycle wearing a light blue long sleeved blouse with closed buttons and dark blue jeans trousers, in the background the exterior of an Aldi store, fully lit background, natural afternoon lighting.
Fig. 11: More visualization results. The first column to the seventh column present
the results of different combinations using LaVi-Bridge, where the language and vision
models used are indicated at the top of each column. The prompts for each row are
displayed either on the right or left.