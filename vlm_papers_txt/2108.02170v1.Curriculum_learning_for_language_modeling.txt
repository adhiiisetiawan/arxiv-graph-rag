CURRICULUM LEARNING FOR LANGUAGE MODELING
Daniel Campos
University of Illinois Urbana-Champaign
dcampos3@illinois.edu
ABSTRACT
Language Models like ELMo and BERT have provided robust representations of natural lan-
guage, which serve as the language understanding component for a diverse range of downstream
tasks.Curriculum learning is a method that employs a structured training regime instead, which has
been leveraged in computer vision and machine translation to improve model training speed and
model performance. While language models have proven transformational for the natural language
processing community, these models have proven expensive, energy-intensive, and challenging to
train. In this work, we explore the effect of curriculum learning on language model pretraining
using various linguistically motivated curricula and evaluate transfer performance on the GLUE
Benchmark. Despite a broad variety of training methodologies and experiments we do not Ô¨Ånd
compelling evidence that curriculum learning methods improve language model training.
Keywords Curriculum Learning Language Modeling
1 Introduction
Seeking to represent natural language, researchers have found language models (LM) with Sesame Street-inspired
names [ 1] [2] [3] to be incredibly effective methods of producing language representations (LR). These LM‚Äôs have
leverage transfer learning by training on a large text corpus to learn a good representation of language which can then be
used in a down steam task like Question Answering or Entity Resolution. While these LMs have shown to be excellent
methods to enable language understanding, the ability to train these models is becoming increasingly computationally
expensive [ 4]. Since model performance is closely tied to the size of training data, model size, and compute used to
train [ 5] the bulk of existing research has focused on scaling these aspects without much focus on increasing efÔ¨Åciency
of training. Seeking to explore what methods could be used to make LM training more efÔ¨Åcient we study the effect of
curriculum learning by training ELMo with a wide variety of curricula.
Curriculum learning (CL) is a training methodology which applies structure to a models training data. CL has been
studied broadly in natural language processing and has been very successful in domains like Neural Machine Translation
(NMT) where CL based models are able to train faster and produce better results [ 6] [7] [8] than unstructured, stochastic
sampling. Focusing on LMs, Xu et al. [ 9] showed that CL can be used in LM Ô¨Ånetuning as a way to improve task
performance. Despite an abundance of work exploring CL and LMs to the best of our knowledge we are the Ô¨Årst to
examine the effect of curriculum learning in LM pre-training and transfer performance.
To evaluate the effect of CL on LMs we train ELMo with a variety of curricula on the wikitext-2 and wikitext-103 [ 10]
without modiÔ¨Åcation of training time or model hyperparameters. We evaluate model performance on the pre-training
task and on the GLUE Benchmark [ 11] building on the work of Competence Based Curriculum Learning [ 12] by
modifying training sampler within the LM to produce a dataset with gradually increasing difÔ¨Åculty2. The contributions
of our work are:
‚Ä¢Exploration of the effects of curriculum learning for language modeling Ô¨Ånding no clear improvement to
models that use curriculum methods for training.
‚Ä¢Experiments suggesting random curriculum in which the structure of the training regime is random can be just
as effective as linguistically motivated methods.
Work done pursing Masters Degree at University of Washington
2Code and results available at https://github.com/spacemanidol/CurriculumLearningForLanguageModelsarXiv:2108.02170v1  [cs.CL]  4 Aug 2021Curriculum Learning for Language Modeling
2 Related Work
2.1 Curriculum Learning
CL subset of training regimes which introduce structure to improve training efÔ¨Åciency, model performance, or model
model robustness by optimizing what kind of information a model has access at each training step. Experiments with
RNNs [ 13] suggested that learning of complex grammatical structure improves when the initial examples the models
learn with are more easier. Experiments in modifying language modeling training data Ô¨Ånd a lower loss can be achieved
by training on incrementally more difÔ¨Åcult data [ 14]. Recently, competence based curriculum [ 6] has been used to
improve machine translation progressively modifying the training corpus until it matches the original distribution. It
has been used to reduce training time by up to 70% and improve BLEU performance by 2.2 points on the WMT dataset.
For further readings about curriculum learning, applications and current bottlenecks we recommend Soviany et al.‚Äôs
survey [15]
2.2 Language Modeling
Language modeling is a way to assign a probability distribution over some textual representation. In other words, if the
task is to model n-grams, the probability of a current input is the probability of a token wigiven the previous itokens.
Language Models like ELMo [ 1] and BERT [ 2] leverage large text corpora to learn language representations that can
be used for downstream tasks like text classiÔ¨Åcation or question answering. While LMs lead to large improvement in
performance for downstream tasks they are both expensive and complex to train. A single training run of a model like
GPT-2 can cost upward of $40,000, the architecture search and hyperparameter tuning can be upwards of $3,000,000,
and the C 02released by training one of these models can be similar to the C 02released in the entire life-cycle of a car
[16].
3 Method
Language modeling is a way to assign a probability distribution over some textual representation. This probability
distribution is commonly modeled as the probability of a current token wigiven the previous itokens as formally
represented in equation 2. Using language modeling as a pre-training method, LMs learn representations which can be
used in downstream tasks. Since language has structure, we believe that structuring the pre-training methodology can
lead to improved model performance. To introduce structure into LM training we leverage Platanios et al.‚Äôs competence
based curriculum (CBC)[ 6] as shown in Algorithm 1. CBC uses a notion of model competence and sample difÔ¨Åculty
to control what a model learns. First, the corpus, X, a collection of samples S, where each sample siis a sequence
of wordssi=wi
o;wi
1;:::;wi
nis sorted by difÔ¨Åculty using a which Using a heuristic like sentence length or unigram
rarity is assigned a difÔ¨Åculty si= [0;1]. Given a processed corpus, a model is assigned a initial competence 0and a
competence increment lambda increment . A model‚Äôs competence score is a representation of how far along in a training
regime the model is. At each training step, a model samples from data that is lower than its current competence, updates
its weights, and increases its competence. The model is only able to train on samples that have a difÔ¨Åculty where
si<=t.
Using CBC we explore 8 proxies for sample difÔ¨Åculty: no curriculum, random, sample length, unigram sample proba-
Algorithm 1: CBC Training Regime
Result: Model Trained with Competence Based Curriculum
Input: X,0,increment ;
Compute difÔ¨Åculty, siforsi2X;
Compute Cumulative density of si;
t=0;
fortraining step t = 1,...,n do
Sample batch bfrom X such that si< t;
Train on batch b;
t+1=t+increment ;
end
bility, bigram sample probability, trigram sample probability, part of speech diversity (POS), and sample dependency
parse complexity (DEP). For each methodology, for each siinX, we compute a difÔ¨Åculty value for each sample si
and then sort the dataset by this difÔ¨Åculty score. Using the sorted dataset we compute the cumulative density function
(CDF) giving each sample of the difÔ¨Åculty score si2[0;1]. No curriculum sets 0= 1which means training samples
2Curriculum Learning for Language Modeling
stochastically and serves as a baseline. A random curriculum is generated by assigning values for siat random and
establishes the effect of any arbitrary structure. The remaining six heuristics are based on common NLP difÔ¨Åculty
metrics and linguistically motivated heuristics.
3.0.1 Sample Length
Sample Length builds on the idea that is a lot harder to model longer sentences, as longer sentences require better
tracking of dependencies. It is calculated by creating a CDF on sentence-length- si=length (si).
3.0.2 Sentence Entropy: N-gram difÔ¨Åculty
Sentence Entropy builds uses the notion that can be difÔ¨Åcult to model is words with a variety of frequency in the
corpora. Models, if assumed to behave like humans, would Ô¨Ånd it difÔ¨Åcult to understand the meaning of a word if
they do not see it in a corpus nor have a diversity of usages to infer meaning. Since the statistical strength of training
samples with rare words is low and the early model learned word embeddings are likely to have high variance it is likely
that exposing a model early to rare words can result in badly estimated representations. To quantify this difÔ¨Åculty we
propose producing a sentence entropy for each sentence with respect to its unigram, bigram, and trigram probabilities.
These are calculated using standard sample entropy calculations as shown below Sample entropy for each N-gram can
be thought of the probability of the sample occurring given an approximate naive language modeling assuming words
are sampled independently. Samples are scored by calculating the product of n-gram log likelihood given the sample.
Note, we are not calculating the conditional probability of each word given the preceding N words but the probability
of the N-gram given the text corpus. Calculation of siis shown in equation 1 where uc,bc, andtcare the counts of
unique unigrams, bigrams, and trigrams in the corpus, Cis the corpus, c(y)is the count of yin a sample, x2Cis a
sample in the corpus and wi2xis a word in a line, and l(x)is the length of xinn-grams.
p(wn) =P
x2Cc(wn)
us
p(wn;wm) =P
x2Cc(wn;wm)
bs
p(wn;wm;wj) =P
x2Cc(wn;wm;wj)
ts
unigram-(si) =l(si)Y
n=0log(p(wn))
bigram-(si) =l(si) 1Y
n=0log(p(wn 1;wn))
trigram-si=l(si) 2Y
n=0log(p(wn;wn+1;wn+2))(1)
3.0.3 Dependency Tree
Sentences are often modeled as dependency trees to model the interaction between words and groups of words in a text
sample. While not infallible, sentences that have a deeper tree usually more complex and as a result more difÔ¨Åcult. We
leverage the language processing framework SPACY .IO‚Äôs to generate parse trees for each sample and measure the depth
of each tree. This information is then used to calculate difÔ¨Åcult as dep-si=depth (si). Since there are fewer unique
values for tree depth this method can be inferred to have a high commonality with random difÔ¨Åculty.
3.1 Part of Speech Diversity
Another core part of language complexity can be derived by the diversity of parts-of-speech in a sentence. We believe
that more difÔ¨Åcult sentences feature a higher diversity of parts-of-speech (POS) and use SPACY .IO‚Äôs part of speech
3Curriculum Learning for Language Modeling
tagger to produce a set for in each sample and calculate difÔ¨Åculty with pos- si=len(set(pos(si)))
P(w1;:::;w m) =mY
i=1P(wijw1;:::;w i 1)
mY
i=1P(wijwi (n 1);:::;w i 1)(2)
4 Experiments
To evaluate the effect of curriculum learning on language modeling we train ELMo models varying the training
corpus and using our aforementioned difÔ¨Åculty proxies to generate various language models. Training leverages
well-established language modeling benchmarks of wikitext-2, and wikitext-103 [ 10] with details can be found in table
1 . These datasets collect veriÔ¨Åed good and featured articles from English Wikipedia and feature 2 million and 103
million tokens and were selected for the variations in size and speed of training. After training, each language model
performance is then evaluated based on performance on the training corpus (measured in perplexity) and transfer ability
on The General Language Understanding Evaluation Benchmark (GLUE) [ 11]. GLUE is a set of resources focused
on the evaluation of natural language understanding systems. This benchmark pools eleven sentence-level language
understanding tasks tasks.
Corpus Name vocabulary Size Tokens lines sentences
wikitext=2 33278 2507007 44836 131262
wikitext-103 267735 103690236 1809468 5343947
1B Word Benchmark 793471 829250940 N/A N/A
Table 1: Training Corpus details
4.1 Pre-Training Details
Using the 16 curricula (8 for each corpus) we train an ELMo model using the original code3with a modiÔ¨Åed batch
sampler created for competence based sampling. For baselines, we train Elmo models without our modiÔ¨Åed CBC
sampling using wikitext-2, wikitext-103. Following the original work, we train each curriculum-model variant for 10
epochs on the pre-training corpus, use 2 stacked 4096 dimensional BiLSTMs, use dropout of 0.1, batch size of 128,
and a context window of 20 tokens. Training was performed using 3 Nvidia 2080 TI GPUs requiring about 30 hours
for the wikitext-103 and about an hour for wikitext-2. For CBC training hyperparameters, we performed a grid search
onincrement and0values Ô¨Ånding the lowest training perplexity at 0= 1e 10= 1e 1for wikitext-2 and
0= 1e 3increment = 1e 5for wikitext-103.
In the original implementation, the training loader loads a Ô¨Åle, shufÔ¨Çe all the lines, and samples batches by iterating
through the shufÔ¨Çed corpus. Our method load the full corpus, then select a batch at random from the examples that meet
our model‚Äôs current competence. This changes data sampling to unconstrained random sampling without replacement to
sampling with replacement. Since our competence based sampling has a variety of sample lengths we use the padding
token<PAD > as is common in NMT. All samples are padded to length 20 and the loss on these padding tokens
is set to zero. Since padding tokens for wikitext-103 we introduce 12,204,311 tokens equating to approximately 12
percent more FLOPs.
4.2 Transfer Learning
After models have pretrained we evaluate on GLUE performance using the JIANT toolkit [ 17]. JIANT is an open-source
tool for conducting multi-task and transfer learning experiments in English to implement the GLUE benchmark. JIANT
builds on the notion of a conÔ¨Åguration which provides all settings needed to run and reproduce an experiment in a
simple text Ô¨Åle. JIANT provides consistent data processing, classiÔ¨Åer implementation, and evaluation to ensure that
users of the framework can focus on the outputs and not worry about implementing benchmarking tasks like GLUE.
JIANT uses the pretrained model weight along with a multi-layer perceptron with 512 hidden dimensions to train
on each GLUE tasks. Each JIANT experiment Ô¨Åxes training identically across tasks and inputs using a batch size of
3https://github.com/allenai/bilm-tf
4Curriculum Learning for Language Modeling
8, random seed of 42 initial learning rate of 1e-1, dropout of 0.2. Training of each model continues until the model
learning rate dips below 1e-6 or the model performance has not improved in 10 epochs. Unless another metric is
explicitly mentioned, the GLUE sub-task metric is accuracy.
4.3 Experimental Results
Focusing on model pretraining performance, despite attempts in variation of 0andincrement , no implementation of
CBC is able to approximate the baselines in term of perplexity on the held out portion of the wikitext-* dataset. Complete
graphs can be found in the appendix but all curricula perplexities including the baseline are order of magnitudes higher
than stochastic sampling. On wikitext-2, the best performance is achieved by the curricula baseline ( 0= 1) with a
perplexity of 770, followed by random with a perplexity of 2105 well above the baseline of 151. We believe this is
caused by the change in dataset distribution caused by our curriculum learning implementation. Similar effects are seen
on wikitext-103 where unlike stochastic sampling, which achieve a perplexity of 36, curriculum methods are unable to
achieve a perplexity under one thousand. Surprisingly, as data size scales we see a larger volatility in perplexity during
training with respect to validation perplexity scores which we attribute to constantly shifting sample distribution caused
by curriculum methods.
As we move our focus to GLUE results on wikitext-2 based models in table 2, we Ô¨Ånd that curriculum methods generally
outperform stochastic sampling by 10%. We do not Ô¨Ånd strong evidence that the structure of the curriculum matters
as non curriculum ( 0= 1) performs better than 4 other curricula and the baseline. Perhaps most surprising, random
outperforms the baseline when measure by overall glue score despite their being no formal structure in the training
regime. Observing variability at an individual task level we Ô¨Ånd only CoLA, STS-B and SST have broad variability in
performance. We believe this is because these tasks are smaller and more linguistically challenging.
Focusing on results on larger corpus in table 3 we Ô¨Ånd trends found in wikitext-2 no longer hold as top performance is
achieved by stochastic unmodiÔ¨Åed baseline. We also note that the orderign of system performance does not hold across
datasets and as the pretraining dataset grows the variability between models decreases. Similar to the smaller corpus,
we Ô¨Ånd the highest sensitivity in ColA and Ô¨Ånd variability in SST and STS-B has become more muted. Surprisingly,
given their had the worse performance in pretraining perplexity, the trigram curricula generate the best transfer task.
Overall, we Ô¨Ånd that CBC training provided worse performance on validation perplexity portion and improved
performance on transfer tasks when he pretraining corpus is small. We believe this reinforces the importance of the size
of the pretraining corpus since a large corpus allows the model to learn better language representations without any
structured training. We also Ô¨Ånd a large disconnect in model pretraining perplexity and transfer task performance as
performance on one is not predicative of the other.
Method Overall Cola SST MRPC STS-B QQP MNLI QNLI RTE WNLI DX
dep 0.63 0.19 0.73 0.85/0.78 0.71/0.71 0.74/0.78 0.60 0.75 0.58 0.56 0.11
unigram 0.63 0.18 0.77 0.86/0.78 0.68/0.67 0.74/0.79 0.60 0.75 0.56 0.54 0.13
trigram 0.63 0.15 0.76 0.84/0.76 0.70/0.69 0.73/0.78 0.62 0.76 0.54 0.56 0.14
length 0.63 0.19 0.75 0.84/0.77 0.66/0.65 0.73/0.78 0.60 0.75 0.57 0.56 0.13
no curricula 0.62 0.15 0.75 0.84/0.77 0.71/0.71 0.73/0.78 0.61 0.72 0.54 0.56 0.12
bigram 0.62 0.18 0.77 0.86/0.78 0.68/0.67 0.74/0.79 0.60 0.75 0.56 0.44 0.13
random 0.61 0.00 0.76 0.85/0.78 0.70/0.70 0.72/0.78 0.61 0.75 0.58 0.56 0.14
pos 0.61 0.00 0.74 0.84/0.77 0.66/0.66 0.71/0.77 0.61 0.75 0.59 0.56 0.16
baseline 0.59 0.00 0.70 0.85/0.78 0.66/0.66 0.70/0.75 0.59 0.72 0.54 0.56 0.13
length 0.53 0.01 0.75 0.81/0.67 0.71/0.71 0.54/0.68 0.33 0.51 0.59 0.52 0.01
Table 2: GLUE results for CBC models trained on wikitext-2.
4.4 Failure of Competence Based Curriculum
In our experiments we were quite surprised at the failure to learn the training data found in our implementation of
competence based curriculum as shown by the high perplexity on the wikitext-* datasets. Based on the changes in
validation perplexity we believe the model is over-Ô¨Åtting on the altered training data. We believe the cause of this is
our hyperparameter selection for 0andincrement . We realize that since each method is effusively sampling from a
different training distribution comparison of training perplexities are not comparable. Additionally, if we look at the
difference in validation perplexity curves of various methods it is apparent that they are not learning at the same rate.
Some methods like DEP, and POS do not see major Ô¨Çuctuations indicating the chosen curriculum parameters work well
while many of the n-gram methods consistently Ô¨Çuctuate in a similar fashion indicating the chosen hyperparameters
are sub optimal for them. Given the non trivial computational cost to explore 0andincrement for each method
5Curriculum Learning for Language Modeling
Method Overall Cola SST MRPC STS-B QQP MNLI QNLI RTE WNLI DX
baseline 0.67 0.28 0.86 0.87/0.80 0.77/0.77 0.72/0.76 0.64 0.76 0.61 0.54 0.14
trigram 0.66 0.21 0.85 0.87/0.80 0.78/0.78 0.75/0.80 0.66 0.77 0.56 0.55 0.14
no curriculum 0.66 0.21 0.83 0.87/0.8 0.77/0.77 0.75/0.79 0.64 0.77 0.58 0.56 0.15
bigram 0.66 0.18 0.83 0.85/0.79 0.77/0.77 0.75/0.79 0.65 0.77 0.56 0.56 0.14
length 0.66 0.21 0.82 0.85/0.72 0.77/0.77 0.73/0.79 0.63 0.75 0.58 0.56 0.14
unigram 0.65 0.19 0.82 0.86/0.79 0.76/0.75 0.75/0.79 0.63 0.75 0.57 0.56 0.13
random 0.65 0.18 0.84 0.86/0.79 0.77/0.77 0.75/0.80 0.64 0.77 0.58 0.49 0.14
pos 0.65 0.16 0.83 0.86/0.79 0.76/0.76 0.75/0.79 0.63 0.73 0.57 0.56 0.14
dep 0.64 0.23 0.85 0.86/0.78 0.78/0.78 0.75/0.79 0.64 0.76 0.54 0.42 0.14
Table 3: GLUE results for CBC methods trained on wikitext-103.
and the disconnect seen between pre-training perplexity and performance on GLUE we did not perform additional
hyperparameter optimization.
5 Conclusion and Future Work
In our work we do not Ô¨Ånd strong evidence that the use of curriculum learning is able to improve language model
pretraining. Our CBC based training regimes are unable to learn a good representation of the training corpus but their
representations transfer well to downstream NLP tasks. We Ô¨Ånd that with a small pretraining corpus, CBC methods can
outperform stochastic sampling but as corpus size scales the beneÔ¨Åt is lost. Moreover we do not Ô¨Ånd any evidence that
any type of heuristic for difÔ¨Åculty to be more apt for CBC.
Leveraging recent work on training BERT in an academic setting [ 18] we will explore how our top performing methods
like trigram and random perform when their code becomes public. Building on promising results in model compression,
[19] [20] we would like to explore how transfer learning effects compression. Inspired by the structure transformer-
based models, we would explore jointly how progressively scaling the number of transformer encoders while increasing
data difÔ¨Åculty, or context windows as this has proven useful for GANs [21].
References
[1]Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. Deep contextualized word representations. ArXiv , abs/1802.05365, 2018.
[2]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In NAACL-HLT , 2019.
[3]Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,
and Hua Wu. Ernie: Enhanced representation through knowledge integration. ArXiv , abs/1904.09223, 2019.
[4]Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in
nlp. In ACL, 2019.
[5]Jean Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv , abs/2001.08361, 2020.
[6]Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnab√°s P√≥czos, and Tom Michael Mitchell.
Competence-based curriculum learning for neural machine translation. ArXiv , abs/1903.09848, 2019.
[7]Xuan Zhang, Pamela Shapiro, Manish Kumar, P. McNamee, Marine Carpuat, and Kevin Duh. Curriculum learning
for domain adaptation in neural machine translation. ArXiv , abs/1905.05816, 2019.
[8]Gustavo Penha and C. Hauff. Curriculum learning strategies for ir. Advances in Information Retrieval , 12035:699
‚Äì 713, 2020.
[9]Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. Curriculum
learning for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 6095‚Äì6104, Online, July 2020. Association for Computational Linguistics.
[10] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. ArXiv ,
abs/1609.07843, 2016.
[11] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task
benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP , 2018.
6Curriculum Learning for Language Modeling
[12] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, and Tom Mitchell. Competence-
based curriculum learning for neural machine translation. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers) , Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
[13] J. Elman. Learning and development in neural networks: the importance of starting small. Cognition , 48:71‚Äì99,
1993.
[14] Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML ‚Äô09 , 2009.
[15] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and N. Sebe. Curriculum learning: A survey. ArXiv ,
abs/2101.10382, 2021.
[16] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning
in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages
3645‚Äì3650, Florence, Italy, July 2019. Association for Computational Linguistics.
[17] Yada Pruksachatkun, Phil Yeres, Haokun Liu, Jason Phang, Phu Mon Htut, Alex Wang, Ian Tenney, and
Samuel R. Bowman. jiant: A software toolkit for research on general-purpose text understanding models. ArXiv ,
abs/2003.02249, 2020.
[18] Peter Izsak, Moshe Berchansky, and Omer Levy. How to train bert with an academic budget. ArXiv ,
abs/2104.07705, 2021.
[19] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
arXiv: Learning , 2019.
[20] Adrian de Wynter and D. Perry. Optimal subarchitecture extraction for bert. ArXiv , abs/2010.10499, 2020.
[21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality,
stability, and variation. ArXiv , abs/1710.10196, 2017.
6 Appendix
6.1 Competence Based Curricula Perplexity Results
Figure 1: Validation perplexity of each curriculum trained on based wikitext-2 measured every 100 batches.
7Curriculum Learning for Language Modeling
Figure 2: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches.
Figure 3: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Unigram,
bigram, and baseline model performance removed improved interpretation
Figure 4: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Bigram
performance is removed for ease of interpretation.
8Curriculum Learning for Language Modeling
Figure 5: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Bigram and
Baseline performance is removed for ease of interpretation.
Figure 6: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Bigram,
trigram, and Baseline performance is removed for ease of interpretation.
Figure 7: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Unigram,
Bigram, Trigram and Baseline performance is removed for ease of interpretation.
9