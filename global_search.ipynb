{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adhi/arxiv-graph-rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.global_search.community_context import (\n",
    "    GlobalCommunityContext,\n",
    ")\n",
    "from graphrag.query.structured_search.global_search.search import GlobalSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet files generated from indexing pipeline\n",
    "INPUT_DIR = \"output/20240711-140302/artifacts\"\n",
    "COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
    "ENTITY_TABLE = \"create_final_nodes\"\n",
    "ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
    "\n",
    "# community level in the Leiden community hierarchy from which we will load the community reports\n",
    "# higher value means we use reports from more fine-grained communities (at the cost of higher computation cost)\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adhi/arxiv-graph-rag/.venv/lib/python3.10/site-packages/graphrag/query/indexer_adapters.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  entity_df[\"community\"] = entity_df[\"community\"].fillna(-1)\n",
      "/home/adhi/arxiv-graph-rag/.venv/lib/python3.10/site-packages/graphrag/query/indexer_adapters.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  entity_df[\"community\"] = entity_df[\"community\"].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report records: 1166\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community</th>\n",
       "      <th>full_content</th>\n",
       "      <th>level</th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>rank_explanation</th>\n",
       "      <th>summary</th>\n",
       "      <th>findings</th>\n",
       "      <th>full_content_json</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100</td>\n",
       "      <td># CLIP and Related Entities\\n\\nThe community r...</td>\n",
       "      <td>3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>CLIP and Related Entities</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>The community revolves around the CLIP model, ...</td>\n",
       "      <td>[{'explanation': 'CLIP serves as a foundation ...</td>\n",
       "      <td>{\\n    \"title\": \"CLIP and Related Entities\",\\n...</td>\n",
       "      <td>7ed1bc71-4032-4cbf-8513-c93d633fb7d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1101</td>\n",
       "      <td># LAION400M and Vision-Language Models\\n\\nThe ...</td>\n",
       "      <td>3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>LAION400M and Vision-Language Models</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>The community revolves around the LAION400M da...</td>\n",
       "      <td>[{'explanation': 'LAION400M plays a crucial ro...</td>\n",
       "      <td>{\\n    \"title\": \"LAION400M and Vision-Language...</td>\n",
       "      <td>be58c54e-1f00-4330-8bbb-8526d004e673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102</td>\n",
       "      <td># VL-T5 and Visual Grounding Tasks\\n\\nThe comm...</td>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>VL-T5 and Visual Grounding Tasks</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>The community is centered around the VL-T5 mod...</td>\n",
       "      <td>[{'explanation': 'VL-T5 is a versatile model t...</td>\n",
       "      <td>{\\n    \"title\": \"VL-T5 and Visual Grounding Ta...</td>\n",
       "      <td>34bfa943-22b1-4594-99a8-c06c3873299b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1103</td>\n",
       "      <td># Language and Vision Encoder Community\\n\\nThe...</td>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Language and Vision Encoder Community</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>The community revolves around the Language Enc...</td>\n",
       "      <td>[{'explanation': 'The Language Encoder plays a...</td>\n",
       "      <td>{\\n    \"title\": \"Language and Vision Encoder C...</td>\n",
       "      <td>2d80382b-3631-4472-8efa-39703c977c98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1104</td>\n",
       "      <td># OFA and Wang, P.\\n\\nThe community revolves a...</td>\n",
       "      <td>3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>OFA and Wang, P.</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>The community revolves around the OFA organiza...</td>\n",
       "      <td>[{'explanation': 'OFA is a versatile organizat...</td>\n",
       "      <td>{\\n    \"title\": \"OFA and Wang, P.\",\\n    \"summ...</td>\n",
       "      <td>1507a244-a1f1-4b83-a349-d52ffb4cd05d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  community                                       full_content  level  rank  \\\n",
       "0      1100  # CLIP and Related Entities\\n\\nThe community r...      3   8.5   \n",
       "1      1101  # LAION400M and Vision-Language Models\\n\\nThe ...      3   8.5   \n",
       "2      1102  # VL-T5 and Visual Grounding Tasks\\n\\nThe comm...      3   7.5   \n",
       "3      1103  # Language and Vision Encoder Community\\n\\nThe...      3   7.5   \n",
       "4      1104  # OFA and Wang, P.\\n\\nThe community revolves a...      3   8.5   \n",
       "\n",
       "                                   title  \\\n",
       "0              CLIP and Related Entities   \n",
       "1   LAION400M and Vision-Language Models   \n",
       "2       VL-T5 and Visual Grounding Tasks   \n",
       "3  Language and Vision Encoder Community   \n",
       "4                       OFA and Wang, P.   \n",
       "\n",
       "                                    rank_explanation  \\\n",
       "0  The impact severity rating is high due to the ...   \n",
       "1  The impact severity rating is high due to the ...   \n",
       "2  The impact severity rating is high due to the ...   \n",
       "3  The impact severity rating is high due to the ...   \n",
       "4  The impact severity rating is high due to the ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The community revolves around the CLIP model, ...   \n",
       "1  The community revolves around the LAION400M da...   \n",
       "2  The community is centered around the VL-T5 mod...   \n",
       "3  The community revolves around the Language Enc...   \n",
       "4  The community revolves around the OFA organiza...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  [{'explanation': 'CLIP serves as a foundation ...   \n",
       "1  [{'explanation': 'LAION400M plays a crucial ro...   \n",
       "2  [{'explanation': 'VL-T5 is a versatile model t...   \n",
       "3  [{'explanation': 'The Language Encoder plays a...   \n",
       "4  [{'explanation': 'OFA is a versatile organizat...   \n",
       "\n",
       "                                   full_content_json  \\\n",
       "0  {\\n    \"title\": \"CLIP and Related Entities\",\\n...   \n",
       "1  {\\n    \"title\": \"LAION400M and Vision-Language...   \n",
       "2  {\\n    \"title\": \"VL-T5 and Visual Grounding Ta...   \n",
       "3  {\\n    \"title\": \"Language and Vision Encoder C...   \n",
       "4  {\\n    \"title\": \"OFA and Wang, P.\",\\n    \"summ...   \n",
       "\n",
       "                                     id  \n",
       "0  7ed1bc71-4032-4cbf-8513-c93d633fb7d3  \n",
       "1  be58c54e-1f00-4330-8bbb-8526d004e673  \n",
       "2  34bfa943-22b1-4594-99a8-c06c3873299b  \n",
       "3  2d80382b-3631-4472-8efa-39703c977c98  \n",
       "4  1507a244-a1f1-4b83-a349-d52ffb4cd05d  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "\n",
    "reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)\n",
    "print(f\"Report records: {len(report_df)}\")\n",
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder = GlobalCommunityContext(\n",
    "    community_reports=reports,\n",
    "    entities=entities,  # default to None if you don't want to use community weights for ranking\n",
    "    token_encoder=token_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder_params = {\n",
    "    \"use_community_summary\": False,  # False means using full community reports. True means using community short summaries.\n",
    "    \"shuffle_data\": True,\n",
    "    \"include_community_rank\": True,\n",
    "    \"min_community_rank\": 0,\n",
    "    \"community_rank_name\": \"rank\",\n",
    "    \"include_community_weight\": True,\n",
    "    \"community_weight_name\": \"occurrence weight\",\n",
    "    \"normalize_community_weight\": True,\n",
    "    \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "    \"context_name\": \"Reports\",\n",
    "}\n",
    "\n",
    "map_llm_params = {\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.0,\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "}\n",
    "\n",
    "reduce_llm_params = {\n",
    "    \"max_tokens\": 2000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000-1500)\n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = GlobalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    max_data_tokens=12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "    map_llm_params=map_llm_params,\n",
    "    reduce_llm_params=reduce_llm_params,\n",
    "    allow_general_knowledge=False,  # set this to True will add instruction to encourage the LLM to incorporate general knowledge in the response, which may increase hallucinations, but could be useful in some use cases.\n",
    "    json_mode=True,  # set this to False if your LLM model does not support JSON mode.\n",
    "    context_builder_params=context_builder_params,\n",
    "    concurrent_coroutines=32,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LXMERT Model Release Information\n",
      "\n",
      "The LXMERT model, a significant advancement in the field of vision-and-language tasks, was introduced in 2019. This model, developed by researchers such as Tan, Bansal, and others, marked a pivotal moment in the community's efforts to enhance question answering systems [Data: Reports (1003, 782, 379, 780, 505, +more)]. The release of LXMERT in 2019 at NeurIPS and other conferences signified a collaborative endeavor by the community to leverage large-scale pretraining and innovative approaches to improve performance on various vision-language tasks [Data: Reports (927, 676, 491, 822, 839, +more)].\n",
      "\n",
      "## Evolution and Significance\n",
      "\n",
      "The LXMERT model's introduction in 2019 at conferences like EMNLP-IJCNLP and ECCV marked a significant milestone in the community's pursuit of cross-modal understanding capabilities [Data: Reports (789, 738, 478, 731, 709, +more)]. Researchers like Tan, Bansal, and Parikh played crucial roles in the development of LXMERT, emphasizing its importance in advancing knowledge in semantic inference, language-vision tasks, and pre-trained models [Data: Entities (2697, 2696, 2695); Reports (896)]. The model's foundational role in the community, as highlighted by its extensions like JOINT LXMERT and PROPOSED MODEL, showcased its versatility and effectiveness in handling diverse linguistic inputs and improving question answering accuracy [Data: Reports (710)].\n",
      "\n",
      "## Collaborative Efforts and Innovations\n",
      "\n",
      "Collaborations between key entities such as Bansal, Yu, Cho, and Yadav underscored the community's commitment to developing advanced models like the self-chained image-language model [Data: Reports (711)]. The Joint LXMERT Model's innovative approach to processing language and visual data simultaneously, with components like the MCM Question Encoder and X-ATT, demonstrated a unique method for question representation and information processing [Data: Reports (740)]. Additionally, the community's engagement with cutting-edge technologies like CLIP pre-training, led by researchers such as LiT Zhai and Radford, showcased a forward-thinking approach to research and potential implications for future developments in the field [Data: Reports (740); Entities (2512); Relationships (4543, 4880, 4879, 4881)].\n"
     ]
    }
   ],
   "source": [
    "result = await search_engine.asearch(\n",
    "    \"When LXMERT model released?\"\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>occurrence weight</th>\n",
       "      <th>content</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Vision-Language Multi-Task Learning Community</td>\n",
       "      <td>0.542373</td>\n",
       "      <td># Vision-Language Multi-Task Learning Communit...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>BERT and Related Entities</td>\n",
       "      <td>0.474576</td>\n",
       "      <td># BERT and Related Entities\\n\\nThe community r...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>Visual Question Answering (VQA) Community</td>\n",
       "      <td>0.471186</td>\n",
       "      <td># Visual Question Answering (VQA) Community\\n\\...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>Vision-Language Models and Researchers</td>\n",
       "      <td>0.461017</td>\n",
       "      <td># Vision-Language Models and Researchers\\n\\nTh...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>IEEECVF Conference on Computer Vision and Patt...</td>\n",
       "      <td>0.457627</td>\n",
       "      <td># IEEECVF Conference on Computer Vision and Pa...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>Association for Computational Linguistics Comm...</td>\n",
       "      <td>0.457627</td>\n",
       "      <td># Association for Computational Linguistics Co...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>LXMERT and Multimodal Question Understanding</td>\n",
       "      <td>0.450847</td>\n",
       "      <td># LXMERT and Multimodal Question Understanding...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28</td>\n",
       "      <td>European Conference on Computer Vision 2022 in...</td>\n",
       "      <td>0.396610</td>\n",
       "      <td># European Conference on Computer Vision 2022 ...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3DVLP and Related Entities</td>\n",
       "      <td>0.366102</td>\n",
       "      <td># 3DVLP and Related Entities\\n\\nThe community ...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>Large Models and Image Datasets Community</td>\n",
       "      <td>0.352542</td>\n",
       "      <td># Large Models and Image Datasets Community\\n\\...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22</td>\n",
       "      <td>OpenAI Research Community and Laion-400m Dataset</td>\n",
       "      <td>0.335593</td>\n",
       "      <td># OpenAI Research Community and Laion-400m Dat...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>34</td>\n",
       "      <td>Researchers in Vision and Language</td>\n",
       "      <td>0.267797</td>\n",
       "      <td># Researchers in Vision and Language\\n\\nThe co...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31</td>\n",
       "      <td>Researchers and Conferences in Computer Vision</td>\n",
       "      <td>0.135593</td>\n",
       "      <td># Researchers and Conferences in Computer Visi...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29</td>\n",
       "      <td>GLUE Benchmark and Language Models</td>\n",
       "      <td>0.118644</td>\n",
       "      <td># GLUE Benchmark and Language Models\\n\\nThe co...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33</td>\n",
       "      <td>Decoupled Weight Decay Regularization Community</td>\n",
       "      <td>0.054237</td>\n",
       "      <td># Decoupled Weight Decay Regularization Commun...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>Research and Development Community</td>\n",
       "      <td>0.040678</td>\n",
       "      <td># Research and Development Community\\n\\nThe co...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>Layer Normalization Researchers Community</td>\n",
       "      <td>0.030508</td>\n",
       "      <td># Layer Normalization Researchers Community\\n\\...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>Chainer Framework and School of Informatics an...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td># Chainer Framework and School of Informatics ...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>Vision-Language Models Community</td>\n",
       "      <td>1.000000</td>\n",
       "      <td># Vision-Language Models Community\\n\\nThe Visi...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>UNICODER-VL and Authors</td>\n",
       "      <td>0.938983</td>\n",
       "      <td># UNICODER-VL and Authors\\n\\nThe community rev...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16</td>\n",
       "      <td>VL Transformers and Vision-Language Tasks</td>\n",
       "      <td>0.738983</td>\n",
       "      <td># VL Transformers and Vision-Language Tasks\\n\\...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>Vision-Language Research Community</td>\n",
       "      <td>0.623729</td>\n",
       "      <td># Vision-Language Research Community\\n\\nThe Vi...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>CLIP and Vision-Language Models Community</td>\n",
       "      <td>0.616949</td>\n",
       "      <td># CLIP and Vision-Language Models Community\\n\\...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13</td>\n",
       "      <td>Advances in Neural Information Processing Syst...</td>\n",
       "      <td>0.471186</td>\n",
       "      <td># Advances in Neural Information Processing Sy...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>LaVi-Bridge Community</td>\n",
       "      <td>0.430508</td>\n",
       "      <td># LaVi-Bridge Community\\n\\nThe LaVi-Bridge com...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>Vision-Language Research Community</td>\n",
       "      <td>0.413559</td>\n",
       "      <td># Vision-Language Research Community\\n\\nThe Vi...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>VALOR and Multi-Modal Understanding Community</td>\n",
       "      <td>0.413559</td>\n",
       "      <td># VALOR and Multi-Modal Understanding Communit...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>Google and Neural Machine Translation</td>\n",
       "      <td>0.393220</td>\n",
       "      <td># Google and Neural Machine Translation\\n\\nThe...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>Liu et al. and Vision-Language Planning</td>\n",
       "      <td>0.227119</td>\n",
       "      <td># Liu et al. and Vision-Language Planning\\n\\nT...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35</td>\n",
       "      <td>Entities and Relationships in Multimodal Dataset</td>\n",
       "      <td>0.223729</td>\n",
       "      <td># Entities and Relationships in Multimodal Dat...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>24</td>\n",
       "      <td>Vision-Language Community</td>\n",
       "      <td>0.169492</td>\n",
       "      <td># Vision-Language Community\\n\\nThe Vision-Lang...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>18</td>\n",
       "      <td>Researchers and Language Models</td>\n",
       "      <td>0.135593</td>\n",
       "      <td># Researchers and Language Models\\n\\nThe commu...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15</td>\n",
       "      <td>Hugo Touvron and Llama Community</td>\n",
       "      <td>0.091525</td>\n",
       "      <td># Hugo Touvron and Llama Community\\n\\nThe comm...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>36</td>\n",
       "      <td>EMNLP-IJCNLP and Researchers</td>\n",
       "      <td>0.067797</td>\n",
       "      <td># EMNLP-IJCNLP and Researchers\\n\\nThe communit...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>21</td>\n",
       "      <td>Researchers and Upcoming Paper Collaboration</td>\n",
       "      <td>0.016949</td>\n",
       "      <td># Researchers and Upcoming Paper Collaboration...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  occurrence weight  \\\n",
       "0    6      Vision-Language Multi-Task Learning Community           0.542373   \n",
       "1   23                          BERT and Related Entities           0.474576   \n",
       "2   19          Visual Question Answering (VQA) Community           0.471186   \n",
       "3   17             Vision-Language Models and Researchers           0.461017   \n",
       "4   37  IEEECVF Conference on Computer Vision and Patt...           0.457627   \n",
       "5    9  Association for Computational Linguistics Comm...           0.457627   \n",
       "6   11       LXMERT and Multimodal Question Understanding           0.450847   \n",
       "7   28  European Conference on Computer Vision 2022 in...           0.396610   \n",
       "8    1                         3DVLP and Related Entities           0.366102   \n",
       "9    8          Large Models and Image Datasets Community           0.352542   \n",
       "10  22   OpenAI Research Community and Laion-400m Dataset           0.335593   \n",
       "11  34                 Researchers in Vision and Language           0.267797   \n",
       "12  31     Researchers and Conferences in Computer Vision           0.135593   \n",
       "13  29                 GLUE Benchmark and Language Models           0.118644   \n",
       "14  33    Decoupled Weight Decay Regularization Community           0.054237   \n",
       "15  30                 Research and Development Community           0.040678   \n",
       "16  20          Layer Normalization Researchers Community           0.030508   \n",
       "17  25  Chainer Framework and School of Informatics an...           0.016949   \n",
       "18  10                   Vision-Language Models Community           1.000000   \n",
       "19   3                            UNICODER-VL and Authors           0.938983   \n",
       "20  16          VL Transformers and Vision-Language Tasks           0.738983   \n",
       "21  12                 Vision-Language Research Community           0.623729   \n",
       "22   2          CLIP and Vision-Language Models Community           0.616949   \n",
       "23  13  Advances in Neural Information Processing Syst...           0.471186   \n",
       "24   7                              LaVi-Bridge Community           0.430508   \n",
       "25   0                 Vision-Language Research Community           0.413559   \n",
       "26  38      VALOR and Multi-Modal Understanding Community           0.413559   \n",
       "27  26              Google and Neural Machine Translation           0.393220   \n",
       "28  27            Liu et al. and Vision-Language Planning           0.227119   \n",
       "29  35   Entities and Relationships in Multimodal Dataset           0.223729   \n",
       "30  24                          Vision-Language Community           0.169492   \n",
       "31  18                    Researchers and Language Models           0.135593   \n",
       "32  15                   Hugo Touvron and Llama Community           0.091525   \n",
       "33  36                       EMNLP-IJCNLP and Researchers           0.067797   \n",
       "34  21       Researchers and Upcoming Paper Collaboration           0.016949   \n",
       "\n",
       "                                              content  rank  \n",
       "0   # Vision-Language Multi-Task Learning Communit...   7.5  \n",
       "1   # BERT and Related Entities\\n\\nThe community r...   8.5  \n",
       "2   # Visual Question Answering (VQA) Community\\n\\...   8.0  \n",
       "3   # Vision-Language Models and Researchers\\n\\nTh...   8.5  \n",
       "4   # IEEECVF Conference on Computer Vision and Pa...   8.5  \n",
       "5   # Association for Computational Linguistics Co...   8.5  \n",
       "6   # LXMERT and Multimodal Question Understanding...   8.5  \n",
       "7   # European Conference on Computer Vision 2022 ...   8.5  \n",
       "8   # 3DVLP and Related Entities\\n\\nThe community ...   8.5  \n",
       "9   # Large Models and Image Datasets Community\\n\\...   8.5  \n",
       "10  # OpenAI Research Community and Laion-400m Dat...   8.5  \n",
       "11  # Researchers in Vision and Language\\n\\nThe co...   7.5  \n",
       "12  # Researchers and Conferences in Computer Visi...   8.5  \n",
       "13  # GLUE Benchmark and Language Models\\n\\nThe co...   8.5  \n",
       "14  # Decoupled Weight Decay Regularization Commun...   8.5  \n",
       "15  # Research and Development Community\\n\\nThe co...   7.5  \n",
       "16  # Layer Normalization Researchers Community\\n\\...   7.5  \n",
       "17  # Chainer Framework and School of Informatics ...   7.5  \n",
       "18  # Vision-Language Models Community\\n\\nThe Visi...   8.5  \n",
       "19  # UNICODER-VL and Authors\\n\\nThe community rev...   8.5  \n",
       "20  # VL Transformers and Vision-Language Tasks\\n\\...   8.5  \n",
       "21  # Vision-Language Research Community\\n\\nThe Vi...   8.5  \n",
       "22  # CLIP and Vision-Language Models Community\\n\\...   8.5  \n",
       "23  # Advances in Neural Information Processing Sy...   8.5  \n",
       "24  # LaVi-Bridge Community\\n\\nThe LaVi-Bridge com...   8.5  \n",
       "25  # Vision-Language Research Community\\n\\nThe Vi...   8.5  \n",
       "26  # VALOR and Multi-Modal Understanding Communit...   8.5  \n",
       "27  # Google and Neural Machine Translation\\n\\nThe...   8.5  \n",
       "28  # Liu et al. and Vision-Language Planning\\n\\nT...   7.5  \n",
       "29  # Entities and Relationships in Multimodal Dat...   7.5  \n",
       "30  # Vision-Language Community\\n\\nThe Vision-Lang...   8.5  \n",
       "31  # Researchers and Language Models\\n\\nThe commu...   7.5  \n",
       "32  # Hugo Touvron and Llama Community\\n\\nThe comm...   8.5  \n",
       "33  # EMNLP-IJCNLP and Researchers\\n\\nThe communit...   7.5  \n",
       "34  # Researchers and Upcoming Paper Collaboration...   8.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the data used to build the context for the LLM responses\n",
    "result.context_data[\"reports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM calls: 3. LLM tokens: 25442\n"
     ]
    }
   ],
   "source": [
    "# inspect number of LLM calls and tokens\n",
    "print(f\"LLM calls: {result.llm_calls}. LLM tokens: {result.prompt_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
